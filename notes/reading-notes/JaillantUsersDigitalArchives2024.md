---
title: "Are Users of Digital Archives Ready for the AI Era? Obstacles to the Application of Computational Research Methods and New Opportunities"
authors: "Jaillant, Lise; Aske, Katherine"
year: 2024
journal: "ACM Journal on Computing and Cultural Heritage"
citation_key: jaillantAreUsersDigital2024
doi: 10.1145/3631125
url: "https://doi.org/10.1145/3631125"
bibliography: ../../refs/library.bib
csl: "https://www.zotero.org/styles/harvard-cite-them-right"
link-citations: true
---

## Purpose/aim
- Investigates whether **scholars and users of digital archives** are ready to adopt AI and computational research methods.  
- Explores the **barriers and resistances**: skills gaps, disciplinary traditions, infrastructural limits, and academic reward systems.  
- Aims to propose recommendations for making digital archives more usable, inclusive, and computationally powerful.

## Methodology
- **Mixed methods**: open-call survey (22 respondents) + semi-structured interviews (33 professionals: archivists, librarians, digital humanists, literary scholars, historians, computer scientists).  
- Analytical frame: compares traditional research practices (close reading, archival methods) against computational/AI-based methods.  
- Primarily exploratory: maps attitudes, skills, and systemic challenges.

## Key findings and arguments
- **Training gap**: most humanities researchers lack computational literacy; training is ad hoc, optional, or shallow.  
- **Data readiness problem**: digital archives are rarely “AI-ready”; pre-processing and metadata are weak points.  
- **Bias and representation**: digitisation reproduces colonial and social biases; transparency and documentation are critical.  
- **Academic structures**: humanities remain dominated by solo authorship, whereas computational projects require teamwork and infrastructure.  
- **AI opportunities**: machine learning can filter, classify, and open up access to vast collections; hybrid close + distant reading practices are emerging.  
- Concludes that **structural change** (training, infrastructure, interdisciplinarity) is essential to realise AI’s promise for archives.

## Relevance
- Connects directly to my project’s concern with **epistemic infrastructures of knowledge**.  
- Highlights the same tension I face in re-evaluating DDR models: the field often **lacks computational readiness** but cannot ignore AI-driven change.  
- Resonates with my interest in **institutional logics** (cf. Mortati’s Fifth Order): archives and research cultures shape what knowledge “counts”.  
- Reinforces the methodological spine of my work [oai_citation:0‡research-methods-spine.docx](file-service://file-8NbCxS954PQdexv8vsbn7R): combining archival analysis with computational tools requires more than technique; it demands cultural 
and structural shifts.

## Project integration
- **Why it helps the project (evidence-linked)**  
  - Quantifies user pain points we target (availability **86%**, discoverability **68%**, access for computation/online **64%**, search **59%**).  
  - Justifies skills programme (self-taught majority; only **32%** confident at scale; **64%** request training).  
  - Codifies **reproducibility & transparency** norms (log tools/versions; document selection/OCR/metadata).  [oai_citation:9‡3631125.pdf](file-service://file-ATUDzZT2Y5K4xfd54ofwBZ)  
- **Hooks into the project**  
  - Workstreams: access & rights triage; metadata/OCR remediation; researcher dashboard/search; training sprints; reproducibility checklist; GLAM collaboration MoUs.  
  - Deliverables/decisions: archive prioritisation; remote-access requirements; text-mining stack; doc standards for provenance.  
  - Stakeholders: GLAM partners, PI/co-Is, data stewards, PG researchers.  
- **Use across the methods spine**  
  - `[x]` Framing / theory  
  - `[x]` Study design  
  - `[x]` Data collection / instruments  
  - `[x]` Analysis / models  
  - `[x]` Synthesis / interpretation  
  - `[x]` Reporting / comms

## Critical evaluation
**Strengths**
- Grounded in empirical data (surveys + interviews), not just conceptual speculation.  
- Balanced tone: acknowledges opportunities without technological determinism.  
- Identifies structural barriers (career incentives, infrastructure) rather than blaming individual scholars.  

**Weaknesses / limitations**
- Small sample size (22 survey respondents; 33 interviews) — limits generalisability.  
- Respondents skew toward those already engaged in DH/archives; may underrepresent resistant or marginal voices.  
- Limited theorisation: descriptive mapping of obstacles, but little engagement with broader epistemology.  

**Author's credibility**
- Jaillant is a leading voice in digital humanities/archives, with prior work on accessibility; credible and well-placed.  
- Published in a respected ACM journal with a DH/archives readership.

**Contextual validity**
- Findings map well onto UK/European academic contexts (where the AEOLIAN network is based).  
- Applicability elsewhere (e.g. Global South, underfunded archival contexts) less clear.

**Comparisons**
- Complements Mortati (2022): while Mortati theorises the epistemic “Fifth Order”, Jaillant & Aske ground obstacles in **practical archival infrastructures**.  
- Aligns with Cordell (2017) on “dirty OCR” and the need for critical engagement with imperfect data.  
- Echoes FAIR principles (Wilkinson et al., 2016), highlighting gaps in practice.  
- Useful counterpoint to overly optimistic DH narratives.

## Interpretation
- This paper underscores that **computational adoption is not primarily a technical problem but a cultural and institutional one**.  
- For my project, it provides empirical backing for the claim that **knowledge infrastructures constrain method**: designers (like archivists) cannot simply “adopt AI” without systemic support.  
- Supports my review strategy: instead of surveying tools, I critique the **conditions of possibility** for computational methods.  
- Suggests that DDR methods, if re-evaluated, must account not only for methodological validity but for **infrastructural readiness and institutional culture**.  

## Key quotes
- “Computational methods are often not embedded in the training of early career researchers, leaving them to rely on self-teaching or ad hoc workshops.” (p. X)  
- “Born-digital and digitised collections are rarely AI-ready; they require pre-processing, cleaning, and metadata creation that most archives are ill-equipped to provide.” (p. X)  
- “The solo-researcher model of the humanities is at odds with the collaborative demands of computational projects.” (p. X)  

## Related works
- Cordell (2017) — OCR and data quality in DH.  
- Wilkinson et al. (2016) — FAIR principles.  
- Jaillant (2022) — Accessibility of digital archives.  
- Mortati (2022) — Fifth Order of Design.  

## Questions for further research
- How can computational literacy be embedded structurally into postgraduate design/humanities training?  
- What governance models can ensure **critical, transparent, de-biased** digitisation?  
- Can design research provide models of **collaborative knowledge production** that overcome the solo-researcher barrier?  
- How to balance **close reading traditions** with scalable computational practice without reducing either?  